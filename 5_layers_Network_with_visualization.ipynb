{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 layers Network with visualization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPeGbKbQO5Rv",
        "outputId": "4bf7f8fb-0488-483c-98f5-05928bc547e1"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import io\n",
        "\n",
        "assert tf.__version__.startswith('2.')\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def plot_to_image(figure):\n",
        "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_grid(images, labels):\n",
        "    \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
        "    # Create a figure to contain the plot.\n",
        "    figure = plt.figure(figsize=(10, 10))\n",
        "    for i in range(25):\n",
        "        # Start next subplot.\n",
        "        plt.subplot(5, 5, i + 1, title=str(np.argmax(labels[i].numpy())))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "\n",
        "    return figure\n",
        "\n",
        "\n",
        "batchsz = 128\n",
        "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
        "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
        "\n",
        "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
        "\n",
        "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "ds_val = ds_val.map(preprocess).batch(batchsz, drop_remainder=True)\n",
        "\n",
        "network = Sequential([layers.Dense(256, activation='relu'),\n",
        "                      layers.Dense(128, activation='relu'),\n",
        "                      layers.Dense(64, activation='relu'),\n",
        "                      layers.Dense(32, activation='relu'),\n",
        "                      layers.Dense(10)])\n",
        "network.build(input_shape=(None, 28 * 28))\n",
        "network.summary()\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = 'logs/' + current_time\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "# get x from (x,y)\n",
        "sample_img = next(iter(db))[0]\n",
        "# get first image instance\n",
        "sample_img = sample_img[0]\n",
        "sample_img = tf.reshape(sample_img, [1, 28, 28, 1])\n",
        "with summary_writer.as_default():\n",
        "    tf.summary.image(\"Training sample:\", sample_img, step=0)\n",
        "\n",
        "for step, (x, y) in enumerate(db):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # [b, 28, 28] => [b, 784]\n",
        "        x = tf.reshape(x, (-1, 28 * 28))\n",
        "        # [b, 784] => [b, 10]\n",
        "        out = network(x)\n",
        "        # [b] => [b, 10]\n",
        "        y_onehot = tf.one_hot(y, depth=10)\n",
        "        # [b]\n",
        "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n",
        "\n",
        "    grads = tape.gradient(loss, network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, 'loss:', float(loss))\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('train-loss', float(loss), step=step)\n",
        "\n",
        "            # evaluate\n",
        "    if step % 500 == 0:\n",
        "        total, total_correct = 0., 0\n",
        "\n",
        "        for _, (x, y) in enumerate(ds_val):\n",
        "            # [b, 28, 28] => [b, 784]\n",
        "            x = tf.reshape(x, (-1, 28 * 28))\n",
        "            # [b, 784] => [b, 10]\n",
        "            out = network(x)\n",
        "            # [b, 10] => [b]\n",
        "            pred = tf.argmax(out, axis=1)\n",
        "            pred = tf.cast(pred, dtype=tf.int32)\n",
        "            # bool type\n",
        "            correct = tf.equal(pred, y)\n",
        "            # bool tensor => int tensor => numpy\n",
        "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
        "            total += x.shape[0]\n",
        "\n",
        "        print(step, 'Evaluate Acc:', total_correct / total)\n",
        "\n",
        "        # print(x.shape)\n",
        "        val_images = x[:25]\n",
        "        val_labels = out[:25]\n",
        "        val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('test-acc', float(total_correct / total), step=step)\n",
        "            tf.summary.image(\"val-onebyone-images:\", val_images, max_outputs=25, step=step)\n",
        "\n",
        "            val_images = tf.reshape(val_images, [-1, 28, 28])\n",
        "            figure = image_grid(val_images, val_labels)\n",
        "            tf.summary.image('val-images:', plot_to_image(figure), step=step)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets: (60000, 28, 28) (60000,) 0 255\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 244,522\n",
            "Trainable params: 244,522\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0 loss: 2.331740140914917\n",
            "0 Evaluate Acc: 0.10506810897435898\n",
            "100 loss: 0.2215707004070282\n",
            "200 loss: 0.1266850084066391\n",
            "300 loss: 0.14158208668231964\n",
            "400 loss: 0.13609284162521362\n",
            "500 loss: 0.14951686561107635\n",
            "500 Evaluate Acc: 0.9520232371794872\n",
            "600 loss: 0.1207992359995842\n",
            "700 loss: 0.11399335414171219\n",
            "800 loss: 0.11146439611911774\n",
            "900 loss: 0.1368892639875412\n",
            "1000 loss: 0.1327918916940689\n",
            "1000 Evaluate Acc: 0.9603365384615384\n",
            "1100 loss: 0.06197434291243553\n",
            "1200 loss: 0.1092875525355339\n",
            "1300 loss: 0.1249944418668747\n",
            "1400 loss: 0.051976729184389114\n",
            "1500 loss: 0.1397446244955063\n",
            "1500 Evaluate Acc: 0.9646434294871795\n",
            "1600 loss: 0.06452324986457825\n",
            "1700 loss: 0.05442114174365997\n",
            "1800 loss: 0.07912838459014893\n",
            "1900 loss: 0.04461934044957161\n",
            "2000 loss: 0.07438182830810547\n",
            "2000 Evaluate Acc: 0.9729567307692307\n",
            "2100 loss: 0.04832030087709427\n",
            "2200 loss: 0.08535683155059814\n",
            "2300 loss: 0.09421131759881973\n",
            "2400 loss: 0.07133707404136658\n",
            "2500 loss: 0.06251850724220276\n",
            "2500 Evaluate Acc: 0.9713541666666666\n",
            "2600 loss: 0.09299546480178833\n",
            "2700 loss: 0.08798995614051819\n",
            "2800 loss: 0.2601824998855591\n",
            "2900 loss: 0.09307149797677994\n",
            "3000 loss: 0.09456335008144379\n",
            "3000 Evaluate Acc: 0.9735576923076923\n",
            "3100 loss: 0.07359510660171509\n",
            "3200 loss: 0.08215556293725967\n",
            "3300 loss: 0.12647214531898499\n",
            "3400 loss: 0.08551289141178131\n",
            "3500 loss: 0.07996602356433868\n",
            "3500 Evaluate Acc: 0.9694511217948718\n",
            "3600 loss: 0.10510119050741196\n",
            "3700 loss: 0.0865255817770958\n",
            "3800 loss: 0.01787358708679676\n",
            "3900 loss: 0.019820349290966988\n",
            "4000 loss: 0.05226278305053711\n",
            "4000 Evaluate Acc: 0.9744591346153846\n",
            "4100 loss: 0.02892833761870861\n",
            "4200 loss: 0.11424229294061661\n",
            "4300 loss: 0.01480804942548275\n",
            "4400 loss: 0.014044606126844883\n",
            "4500 loss: 0.045073699206113815\n",
            "4500 Evaluate Acc: 0.9728565705128205\n",
            "4600 loss: 0.08621268719434738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAsXB0h8eNij"
      },
      "source": [
        "# pip install --upgrade tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "talnwdjKe-45",
        "outputId": "2fa8fb12-a72b-48d3-eb48-1331193c6d15"
      },
      "source": [
        "tensorboard --logdir path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4e7ed42b6995>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir path\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}